{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c53ce78-83f6-4074-8fdf-2ca74ca98888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad60f99-242d-405e-835e-db48b97f2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.pardir, '175Project')\n",
    "\n",
    "COL_NAMES = ['character', 'browsing_page_url', 'word_url', 'word', 'definition', 'sentence']\n",
    "\n",
    "def load_urban_dataset():\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(os.path.join(DATA_DIR, 'Urban')):\n",
    "        for f in files:\n",
    "            if f.endswith('.csv') and f.startswith('urban_data'):\n",
    "                file_paths.append(os.path.join(root, f))\n",
    "    df_urban = pd.concat([pd.read_csv(f, names=COL_NAMES) for f in file_paths])\n",
    "\n",
    "    df_nulls = df_urban[(df_urban.isnull().any(axis=1)) | (df_urban.isna().any(axis=1))]\n",
    "    df_urban = df_urban.drop(df_nulls.index)\n",
    "\n",
    "    return df_urban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2293662-143e-41a8-87a9-d34717dff462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of urban dictionary dataset: (2175494, 6)\n",
      "Word:  ALWAYS4TUESDAYS\n",
      "Meaning:  Anything  that is  yang   in your life\n",
      "Sentence:  Damn homie ! That  Big Mac  was  always4tuesdays !!\n"
     ]
    }
   ],
   "source": [
    "urban_dictionary = load_urban_dataset()\n",
    "print(f\"Shape of urban dictionary dataset: {urban_dictionary.shape}\")\n",
    "ud_sample = urban_dictionary[['word', 'definition', 'sentence']].sample(1)\n",
    "for i in ud_sample.values:\n",
    "    print(\"Word: \", i[0])\n",
    "    print(\"Meaning: \", i[1])\n",
    "    print(\"Sentence: \", i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d4c9ee-1492-4b97-a44a-ce6d5ec7321b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                                                   rawrivan\n",
      "definition                                 Awesomely cool   guy\n",
      "sentence      Did you   see  rawrivan? I  want to  be just l...\n",
      "Name: 17480, dtype: str\n",
      "\n",
      "The full item\n",
      "\n",
      "<ArrowStringArray>\n",
      "[                                              'rawrivan',\n",
      "                                   'Awesomely cool   guy',\n",
      " 'Did you   see  rawrivan? I  want to  be just like him.']\n",
      "Length: 3, dtype: str\n"
     ]
    }
   ],
   "source": [
    "urban_data = urban_dictionary[['word', 'definition', 'sentence']]\n",
    "train_u, test_u = train_test_split(urban_data, test_size=0.2, random_state=42, shuffle=True)\n",
    "#example of what the data looks like\n",
    "row = train_u.iloc[0]\n",
    "print(row)\n",
    "print()\n",
    "print(\"The full item\")\n",
    "print()\n",
    "print(row.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d0cde9-3b88-454e-af6d-4c5940029a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to /Users/hidemk/.convokit/saved-corpora/movie-corpus\n",
      "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n",
      "Number of Speakers: 9035\n",
      "Number of Utterances: 304713\n",
      "Number of Conversations: 83097\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "corpus = Corpus(filename=download(\"movie-corpus\"))\n",
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06e530b-22e0-4c4b-ace8-8f173746e694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 9035\n",
      "Number of Utterances: 304713\n",
      "Number of Conversations: 83097\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112afb88-b3f2-4e14-8953-a094fcb033d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total conversations: 83097\n",
      "Example conversation: ['They do not!', 'They do to!']\n"
     ]
    }
   ],
   "source": [
    "conversations_raw = []\n",
    "\n",
    "# Iterate over all conversation IDs\n",
    "for convo_id in corpus.get_conversation_ids():\n",
    "    convo = corpus.get_conversation(convo_id)\n",
    "    # Extract the textual content of each utterance in the conversation\n",
    "    convo_text = [utt.text for utt in convo.iter_utterances()]\n",
    "    conversations_raw.append(convo_text)\n",
    "\n",
    "print(f\"Total conversations: {len(conversations_raw)}\")\n",
    "print(\"Example conversation:\", conversations_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b4cdb9d-5106-48b4-8c7b-b72d3790fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 221616\n",
      "Sample pair: ('They do not!', 'They do to!')\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "for convo in conversations_raw:\n",
    "    for i in range(len(convo)-1):\n",
    "        pairs.append((convo[i], convo[i+1]))\n",
    "\n",
    "print(f\"Total pairs: {len(pairs)}\")\n",
    "print(\"Sample pair:\", pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a0bfad-1b14-421f-a80a-802827967482",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_convos, test_convos = train_test_split(conversations_raw, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a2cc4bf-ce56-49bb-88bb-3a1c73b5c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total utterances: 304713\n",
      "Example utterances: ['They do not!', 'They do to!', 'I hope so.', 'She okay?', \"Let's go.\"]\n"
     ]
    }
   ],
   "source": [
    "all_texts = [utt.text for utt in corpus.iter_utterances()]\n",
    "\n",
    "print(f\"Total utterances: {len(all_texts)}\")\n",
    "print(\"Example utterances:\", all_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e540d87-0eed-46a7-a10f-ec9c4bd826ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts = train_test_split(all_texts, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d2fb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hidemk/miniconda3/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W0212 19:23:18.224000 77207 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Batches: 100%|██████████| 2078/2078 [01:32<00:00, 22.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (66477, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(train_convos, show_progress_bar=True)\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e239299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  0.1206,  0.0736,  ...,  0.1332,  0.0051,  0.0891],\n",
      "        [ 0.1206,  1.0000,  0.1462,  ..., -0.0054, -0.0103,  0.0212],\n",
      "        [ 0.0736,  0.1462,  1.0000,  ...,  0.1846,  0.0885,  0.2024],\n",
      "        ...,\n",
      "        [ 0.1332, -0.0054,  0.1846,  ...,  1.0000,  0.1002,  0.1454],\n",
      "        [ 0.0051, -0.0103,  0.0885,  ...,  0.1002,  1.0000,  0.1599],\n",
      "        [ 0.0891,  0.0212,  0.2024,  ...,  0.1454,  0.1599,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2accfa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "def retrieve(query, k=2):\n",
    "    distances, indices = index.search(embeddings, k)\n",
    "    return [train_convos[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d44394dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b91d9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query):\n",
    "    retrieved_docs = retrieve(query)\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an assistant trying to utilize slang.\n",
    "    Use only the context below to generate slang usage.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, temperature=0.1,max_length=50, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4dae2e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "answer = generate_response(\"Generate a usage for a slang word that you came across in the training data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
